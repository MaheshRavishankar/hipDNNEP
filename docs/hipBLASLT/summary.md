# hipBLAS-LT MatMul/Gemm Support - Implementation Summary

## Overview

Added support for ONNX `MatMul` and `Gemm` operations using AMD's hipBLAS-LT library. This enables accelerated matrix multiplication on AMD GPUs through the hipDNN Execution Provider.

## Architecture

### Design Decision

Extended the existing `Kernel` class rather than creating a separate BlasKernel. The `Kernel` class now routes to either:
- **hipDNN graph path**: For Conv2D operations
- **hipBLAS-LT path**: For MatMul/Gemm operations (via `BlasGraph`)

### Key Components

```
Kernel
  ├── hipDNN Graph (for Conv2D)
  │     └── hipdnn_frontend::graph::Graph
  └── BlasGraph (for MatMul/Gemm)
        └── BlasGraphImpl (pimpl - hides hipBLAS-LT types)
```

## Files

| File | Description |
|------|-------------|
| `include/hipdnn_ep/blas_graph.h` | BlasGraph class declaration, `hipblaslt_handle_t` typedef |
| `src/blas_graph.cc` | hipBLAS-LT implementation with pimpl pattern |
| `src/ep.cc` | `IsSupportedMatMul()`, `IsSupportedGemm()`, handle lifecycle |
| `src/kernel.cc` | Routes MatMul/Gemm to BlasGraph |
| `CMakeLists.txt` | hipBLAS-LT detection, HIP language compilation |

## Row-Major to Column-Major Conversion

hipBLAS-LT (like cuBLAS) uses column-major storage, while ONNX uses row-major. The conversion uses the identity:

```
Row-major: C = A @ B
         ⟺
Column-major: C^T = B^T @ A^T
```

Since row-major data appears transposed when interpreted as column-major, we:
1. **Swap operands**: Pass B first, then A to hipBLAS-LT
2. **Swap dimensions**: Initialize with (N, M, K) instead of (M, N, K)
3. **Swap transpose flags**: For Gemm, transA becomes transB and vice versa

## Supported Operations

### MatMul
- Inputs: A[M,K], B[K,N]
- Output: Y[M,N] = A @ B
- Constraints: 2D matrices only, static shapes, float32/float16

### Gemm
- Inputs: A, B, C (optional bias)
- Output: Y = alpha * op(A) @ op(B) + beta * C
- Attributes: transA, transB, alpha, beta
- Constraints: 2D matrices, static shapes, float32/float16, no broadcasting for C

## Build Configuration

hipBLAS-LT support is optional and auto-detected:

```cmake
find_package(hipblaslt CONFIG)
if(hipblaslt_FOUND)
  # Enable MatMul/Gemm support
  target_compile_definitions(hipdnn_ep PRIVATE HIPDNN_EP_HAS_HIPBLASLT)
endif()
```

The `blas_graph.cc` file is compiled with HIP language (hipcc) to access hipBLAS-LT headers.

## Runtime Behavior

- If hipBLAS-LT is available: MatMul/Gemm ops are claimed and executed on GPU
- If hipBLAS-LT is unavailable: MatMul/Gemm ops fall back to CPU EP
- Detection uses null checks on `hipblaslt_handle_` rather than `#ifdef` where possible

## Tests

| Test | Description |
|------|-------------|
| `BasicMatMul` | Simple A @ B |
| `BasicGemm` | Gemm without bias |
| `GemmWithBias` | Gemm with C input |
| `GemmWithTransposeA` | Gemm with transA=1 |
| `GemmWithTransposeB` | Gemm with transB=1 |
| `GemmWithScaling` | Gemm with alpha/beta != 1 |
| `ReferenceMatMulCorrectness` | CPU reference validation |

Test models are generated by `test/gen_matmul_model.py`.

## Limitations

- 2D matrices only (no batched matmul)
- Static shapes required
- float32 and float16 data types
- No broadcasting for Gemm's C (bias) input
